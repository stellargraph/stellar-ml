{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification of reddit user posts using graph machine learning with Stellargraph\n",
    "\n",
    "We apply the graph machine algorithm APPNP [1] to the task of classifying reddit user posts into 41 different categories using the dataset published in [2] which can be downloaded [here](http://snap.stanford.edu/graphsage/reddit.zip).  \n",
    "\n",
    "The following is a description of the dataset [2]:\n",
    "\n",
    ">Reddit is a large online discussion forum where users post and comment on content in different topical\n",
    ">communities. We constructed a graph dataset from Reddit posts made in the month of September, 2014. The node label in this case is the community, or “subreddit”, that a post belongs to. We sampled\n",
    ">50 large communities and built a post-to-post graph, connecting posts if the same user comments\n",
    ">on both. In total this dataset contains 232,965 posts with an average degree of 492. We use the first\n",
    ">20 days for training and the remaining days for testing (with 30% used for validation). For features,\n",
    ">we use off-the-shelf 300-dimensional GloVe CommonCrawl word vectors [3]; for each post, we\n",
    ">concatenated (i) the average embedding of the post title, (ii) the average embedding of all the post’s\n",
    ">comments (iii) the post’s score, and (iv) the number of comments made on the post.\n",
    "\n",
    "\n",
    "We demonstrate the advantage of using graph features and the scalability of the APPNP algorithm for node classification on the reddit dataset.  We first train a MLP on the node features and then propagate this model using APPNP.  Training is only done on the node features, this approach allows model training to be completed in under a 1 minute on an 8th gen quad-core i7.\n",
    "\n",
    "\n",
    "**References**\n",
    "\n",
    "1. Predict then propagate: Graph neural networks meet personalized pagerank. J. Klicpera,  A. Bojchevski, & S. Günnemann arxiv:1810.05997, 2018.\n",
    "\n",
    "\n",
    "2. Inductive Representation Learning on Large Graphs. W.L. Hamilton, R. Ying, and J. Leskovec arXiv:1706.02216 [cs.SI], 2017.\n",
    "\n",
    "\n",
    "3. Glove: Global vectors for word representation. J. Pennington, R. Socher, and C. D. Manning. In EMNLP, 2014."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import os\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "import stellargraph as sg\n",
    "from stellargraph.mapper import GraphSAGENodeGenerator, FullBatchNodeGenerator, SparseFullBatchNodeSequence\n",
    "from stellargraph.layer import GraphSAGE, GCN, GAT, APPNP\n",
    "from stellargraph.layer.appnp import APPNPPropagationLayer\n",
    "from stellargraph import globalvar\n",
    "from stellargraph.core.utils import GCN_Aadj_feats_op\n",
    "\n",
    "from tensorflow.keras import layers, optimizers, losses, metrics, Model, models\n",
    "from sklearn import preprocessing, feature_extraction\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import metrics\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Reddit Dataset\n",
    "\n",
    "First, we load the reddit dataset which is stored as a series of json files.  We first load the graph data and then the node features and labels and ensure that indexing is consistent across the graph, labels, and node features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.expanduser(\"~/data/reddit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, \"reddit-G.json\")) as gfile:\n",
    "    graph_data = json.load(gfile)\n",
    "    \n",
    "list_node_ids = list(d['id'] for d in graph_data['nodes'])\n",
    "\n",
    "edge_generator = ((link['source'], link['target']) for link in graph_data['links'])\n",
    "edge_df = pd.DataFrame(edge_generator, columns=[\"target\", \"source\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 231443\n",
      "Number of edges: 11606919\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of nodes:\", len(list_node_ids))\n",
    "print(\"Number of edges:\", len(edge_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, \"reddit-class_map.json\")) as tfile:\n",
    "    labels = json.load(tfile)\n",
    "\n",
    "feats = np.load(data_dir + \"/reddit-feats.npy\")\n",
    "\n",
    "feats[:,0] = np.log(feats[:,0]+1.0)\n",
    "feats[:,1] = np.log(feats[:,1]-min(np.min(feats[:,1]), -1))\n",
    "\n",
    "feat_id_map = json.load(open(data_dir + \"/reddit-id_map.json\"))\n",
    "\n",
    "# sort node features to match the order of feat_id_map\n",
    "sorted_idxs = np.array([feat_id_map[key] for key in list_node_ids])\n",
    "feats = feats[sorted_idxs,:]\n",
    "\n",
    "#sort node labnels to match the order of feat_id_map\n",
    "labels = np.array([labels[key] for key in list_node_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_data = pd.DataFrame(feats)\n",
    "\n",
    "target_encoding = preprocessing.OneHotEncoder(sparse=False, categories='auto')\n",
    "targets = target_encoding.fit_transform(labels.reshape(-1, 1))\n",
    "targets = pd.DataFrame(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then split the data into train/val/test based on the labels stored in the dataset. This is the same train/val/test split used in the graphsage paper. Then we fit a standard scaler on only the training data and use it to standardize all of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152410 training nodes.\n",
      "23699 validation nodes.\n",
      "55334 testing nodes.\n"
     ]
    }
   ],
   "source": [
    "def map_node_to_split(node):\n",
    "    if node['test']:\n",
    "        return 'test'\n",
    "    elif node['val']:\n",
    "        return 'val'\n",
    "    else:\n",
    "        return 'train'\n",
    "\n",
    "    \n",
    "train_test_val_dict = dict(zip(list_node_ids, map(map_node_to_split, graph_data['nodes'])))\n",
    "\n",
    "train_mask = [(train_test_val_dict[key] == 'train') for key in list_node_ids]\n",
    "val_mask = [(train_test_val_dict[key] == 'val') for key in list_node_ids]\n",
    "test_mask = [(train_test_val_dict[key] == 'test') for key in list_node_ids]\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(node_data[train_mask].values)\n",
    "node_data.iloc[:, :] = scaler.transform(node_data.values)\n",
    "\n",
    "train_data, train_targets = node_data[train_mask], targets[train_mask]\n",
    "val_data, val_targets = node_data[val_mask], targets[val_mask]\n",
    "test_data, test_targets = node_data[test_mask], targets[test_mask]\n",
    "\n",
    "print(\"{} training nodes.\".format(len(train_data)))\n",
    "print(\"{} validation nodes.\".format(len(val_data)))\n",
    "print(\"{} testing nodes.\".format(len(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an adjacnecy matrix and StellarGraph generator\n",
    "\n",
    "Standard stellargaph workflows involve creating a stellargaph object and using this to create the data generators. For the large reddit graph this requires approximately 20GB of RAM. This demo has a `low_memory` option which when set to `True` creates the data generator directly from the node features and edge list and only requires approximately 8GB of RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_memory = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_adj_from_edgelist(edge_df, nodelist):\n",
    "    '''\n",
    "    This function creates an adjacency matrix directly from an edgelist and a nodelist.\n",
    "    '''\n",
    "    \n",
    "    node_index = dict(zip(nodelist, range(len(nodelist))))\n",
    "    \n",
    "    rows = [node_index[source_node] for source_node in edge_df[\"source\"]]\n",
    "    cols = [node_index[target_node] for target_node in edge_df[\"target\"]]\n",
    "    data = np.ones(len(edge_df), np.float64)\n",
    "    \n",
    "    Aadj = coo_matrix(\n",
    "                (data, (rows, cols)),\n",
    "                shape=(len(nodelist), len(nodelist)),\n",
    "            )\n",
    "    \n",
    "    return Aadj\n",
    "\n",
    "\n",
    "def create_generator_from_edge_df_feats(edge_df, node_data):\n",
    "    '''\n",
    "    This function is a hack that creates stellargaph FullBatchNodeGenerator directly\n",
    "    from an adjacency list and a data frame of node features.\n",
    "    '''\n",
    "    \n",
    "    # create adjacency matrix\n",
    "    \n",
    "    Aadj = create_adj_from_edgelist(edge_df, node_data.index)\n",
    "\n",
    "    _, Aadj = GCN_Aadj_feats_op(\n",
    "                    features=node_data, A=Aadj, method='gcn'\n",
    "        )\n",
    "    \n",
    "    # create a dummy StellarGraph object\n",
    "    gnx = nx.Graph()\n",
    "    gnx.add_nodes_from([\"a\", \"b\"])\n",
    "\n",
    "    gnx.add_edges_from([(\"a\", \"b\")])\n",
    "\n",
    "    gnx = gnx.to_undirected()\n",
    "    features = np.array([[1, 1], [1, 0]])\n",
    "\n",
    "    nodes = gnx.nodes()\n",
    "    features = pd.DataFrame.from_dict(\n",
    "        {n: f for n, f in zip(nodes, features)}, orient=\"index\"\n",
    "    )\n",
    "\n",
    "    G = sg.StellarGraph(gnx, node_type_name=\"node\", node_features=features)\n",
    "    \n",
    "    # create a FullBatchNodeGenerator from the dummy StellarGraph object\n",
    "    generator = FullBatchNodeGenerator(G, method=\"gcn\", sparse=True)\n",
    "    \n",
    "    # manually set FullBatchNodeGenerator attributes\n",
    "    generator.features = node_data\n",
    "    generator.Aadj = Aadj\n",
    "    generator.node_list = list(node_data.index.values)\n",
    "    \n",
    "    return generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GCN (local pooling) filters...\n",
      "Using GCN (local pooling) filters...\n"
     ]
    }
   ],
   "source": [
    "if low_memory == True:\n",
    "    \n",
    "    generator = create_generator_from_edge_df_feats(edge_df, node_data)\n",
    "\n",
    "else:\n",
    "    gnx = nx.from_pandas_edgelist(edgelist)\n",
    "    G = sg.StellarGraph(gnx, node_features=node_data)\n",
    "\n",
    "    gnx.clear()\n",
    "    generator = FullBatchNodeGenerator(G, method=\"gcn\", sparse=True)\n",
    "\n",
    "test_gen = generator.flow(test_data.index, test_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "We now create a MLP and train on the node features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_layer = layers.Input(shape=(train_data.shape[-1]))\n",
    "\n",
    "layer = layers.Dense(512, activation='relu', kernel_regularizer=\"l2\")(in_layer)\n",
    "layer = layers.Dropout(0.5)(layer)\n",
    "layer = layers.Dense(512, activation='relu', kernel_regularizer=\"l2\")(in_layer)\n",
    "layer = layers.Dropout(0.5)(layer)\n",
    "\n",
    "#note the dimension of the output should equal the number of classes to predict!\n",
    "layer = layers.Dense(train_targets.shape[-1], activation='softmax')(layer)\n",
    "\n",
    "fully_connected_model = Model(inputs=in_layer, outputs=layer)\n",
    "\n",
    "fully_connected_model.compile(\n",
    "    optimizer=optimizers.Adam(lr=0.0001),\n",
    "    loss=losses.categorical_crossentropy,\n",
    "    metrics=[\"acc\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    }
   ],
   "source": [
    "history = fully_connected_model.fit(\n",
    "    train_data, train_targets,\n",
    "    epochs = 60,\n",
    "    validation_data = (val_data, val_targets),\n",
    "    batch_size = 300,\n",
    "    verbose = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MLP attains an accuracy of ~70% on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "55334/55334 [==============================] - 2s 39us/sample - loss: 1.1709 - acc: 0.7238\n",
      "\n",
      "Test Set Metrics:\n",
      "\tloss: 1.1709\n",
      "\tacc: 0.7238\n"
     ]
    }
   ],
   "source": [
    "test_metrics = fully_connected_model.evaluate(test_data, test_targets)\n",
    "print(\"\\nTest Set Metrics:\")\n",
    "for name, val in zip(fully_connected_model.metrics_names, test_metrics):\n",
    "    print(\"\\t{}: {:0.4f}\".format(name, val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## APPNP Propagation\n",
    "\n",
    "We now create an APPNP model and propagate the MLP. No further training is happening in this step.  We then test the propagated model on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "appnp = APPNP(layer_sizes=[train_targets.shape[-1]], \n",
    "              activations=['relu'], \n",
    "              bias=True,\n",
    "              generator=generator, \n",
    "              teleport_probability=0.2, \n",
    "              dropout=0.5, \n",
    "              kernel_regularizer='l2'\n",
    ")\n",
    "\n",
    "x_inp, x_out = appnp.propagate_model(fully_connected_model)\n",
    "predictions = layers.Softmax()(x_out)\n",
    "\n",
    "propagated_model = Model(inputs=x_inp, outputs=predictions)\n",
    "propagated_model.compile(loss='categorical_crossentropy', metrics=['acc'],\n",
    "                  optimizer='Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Set Metrics:\n",
      "\tloss: 3.5073\n",
      "\tacc: 0.8947\n"
     ]
    }
   ],
   "source": [
    "test_metrics = propagated_model.evaluate_generator(test_gen)\n",
    "print(\"\\nTest Set Metrics:\")\n",
    "for name, val in zip(propagated_model.metrics_names, test_metrics):\n",
    "    print(\"\\t{}: {:0.4f}\".format(name, val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Propagating the MLP with APPNP increases the test set accuracy by ~15% without any further training. As we are performing single-label multiclass classification the accuracy is equivalent to the micro F1 metric. This micro F1 is comparable to that attained in the GraphSAGE paper [2]. GraphSAGE with LSTM aggregation attains a best supervised F1 of 0.95 in [2], however APPNP only required ~3 minutes of training on an 8th gen i7 compared to the hours required for GraphSAGE while still attaining a similar F1."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
