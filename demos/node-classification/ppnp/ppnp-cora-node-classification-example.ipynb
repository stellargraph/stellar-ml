{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stellargraph example: Personalised Propagation of Neural Predictions (PPNP) and Approximate PPNP (APPNP) on the CORA citation dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import NetworkX and stellargraph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import stellargraph as sg\n",
    "from stellargraph.mapper import FullBatchNodeGenerator\n",
    "from stellargraph.layer.ppnp import PPNP\n",
    "from stellargraph.layer.appnp import APPNP\n",
    "\n",
    "from tensorflow.keras import layers, optimizers, losses, metrics, Model\n",
    "from sklearn import preprocessing, feature_extraction, model_selection\n",
    "from stellargraph import datasets\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the CORA network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.Cora()\n",
    "display(HTML(dataset.description))\n",
    "dataset.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the graph from edgelist (in the order `cited-paper` <- `citing-paper`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edgelist = pd.read_csv(\n",
    "    os.path.join(dataset.data_directory, \"cora.cites\"),\n",
    "    sep=\"\\t\",\n",
    "    header=None,\n",
    "    names=[\"target\", \"source\"],\n",
    ")\n",
    "edgelist[\"label\"] = \"cites\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gnx = nx.from_pandas_edgelist(edgelist, edge_attr=\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.set_node_attributes(Gnx, \"paper\", \"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the features and subject for the nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = [\"w_{}\".format(ii) for ii in range(1433)]\n",
    "column_names = feature_names + [\"subject\"]\n",
    "node_data = pd.read_csv(\n",
    "    os.path.join(dataset.data_directory, \"cora.content\"),\n",
    "    sep=\"\\t\",\n",
    "    header=None,\n",
    "    names=column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We aim to train a graph-ML model that will predict the \"subject\" attribute on the nodes. These subjects are one of 7 categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(node_data[\"subject\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For machine learning we want to take a subset of the nodes for training, and use the rest for validation and testing. We'll use scikit-learn again to do this.\n",
    "\n",
    "Here we're taking 140 node labels for training, 500 for validation, and the rest for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = model_selection.train_test_split(\n",
    "    node_data, train_size=140, test_size=None, stratify=node_data[\"subject\"]\n",
    ")\n",
    "val_data, test_data = model_selection.train_test_split(\n",
    "    test_data, train_size=500, test_size=None, stratify=test_data[\"subject\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note using stratified sampling gives the following counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(train_data[\"subject\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training set has class imbalance that might need to be compensated, e.g., via using a weighted cross-entropy loss in model training, with class weights inversely proportional to class support. However, we will ignore the class imbalance in this example, for simplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting to numeric arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our categorical target, we will use one-hot vectors that will be fed into a soft-max Keras layer during training. To do this conversion ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_encoding = feature_extraction.DictVectorizer(sparse=False)\n",
    "\n",
    "train_targets = target_encoding.fit_transform(train_data[[\"subject\"]].to_dict(\"records\"))\n",
    "val_targets = target_encoding.transform(val_data[[\"subject\"]].to_dict(\"records\"))\n",
    "test_targets = target_encoding.transform(test_data[[\"subject\"]].to_dict(\"records\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now do the same for the node attributes we want to use to predict the subject. These are the feature vectors that the Keras model will use as input. The CORA dataset contains attributes 'w_x' that correspond to words found in that publication. If a word occurs more than once in a publication the relevant attribute will be set to one, otherwise it will be zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_features = node_data[feature_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the PPNP model in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create a StellarGraph object from the NetworkX graph and the node features and targets. It is StellarGraph objects that we use in this library to perform machine learning tasks on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = sg.StellarGraph(Gnx, node_features=node_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(G.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To feed data from the graph to the Keras model we need a generator. Since PPNP is a full-batch model, we use the `FullBatchNodeGenerator` class to feed node features and the normalized graph Laplacian matrix to the model.\n",
    "\n",
    "Specifying the `method='ppnp'` argument to the `FullBatchNodeGenerator` will pre-process the adjacency matrix and supply the personalized page rank matrix necessary for PPNP.  The personalized page rank matrix is a dense matrix and so `sparse=False` must be passed to `FullBatchNodeGenerator`. `teleport_probability=0.1` specifies the probability of returning to the starting node in the propogation step as desribed in the paper (alpha in the paper). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = FullBatchNodeGenerator(\n",
    "    G, method=\"ppnp\", sparse=False, teleport_probability=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training we map only the training nodes returned from our splitter and the target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = generator.flow(train_data.index, train_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can specify our machine learning model, we need a few more parameters for this:\n",
    "\n",
    " * the `layer_sizes` is a list of hidden feature sizes of each full fully connected layer in the model. In this example we use three fully connected layers with 64,64, and 7 hidden node features at each layer. \n",
    " * `activations` is a list of activations applied to each layer's output\n",
    " * `dropout=0.5` specifies a 50% dropout at each layer. \n",
    " * `kernel_regularizer=keras.regularizers.l2(0.001)` specifies a penality that prevents the model weights from become too large and helps limit overfitting\n",
    " \n",
    " #### Note that the size of the final fully connected layer must be equal to the number of classes you are trying to predict.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a PPNP model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppnp = PPNP(\n",
    "    layer_sizes=[64, 64, train_targets.shape[-1]],\n",
    "    activations=[\"relu\", \"relu\", \"relu\"],\n",
    "    generator=generator,\n",
    "    dropout=0.5,\n",
    "    kernel_regularizer=keras.regularizers.l2(0.001),\n",
    ")\n",
    "\n",
    "x_inp, x_out = ppnp.build()\n",
    "predictions = keras.layers.Softmax()(x_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create the actual Keras model with the input tensors `x_inp` and output tensors being the predictions `predictions` from the final dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppnp_model = Model(inputs=x_inp, outputs=predictions)\n",
    "ppnp_model.compile(\n",
    "    optimizer=optimizers.Adam(lr=0.01),\n",
    "    loss=losses.categorical_crossentropy,\n",
    "    metrics=[\"acc\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model, keeping track of its loss and accuracy on the training set, and its generalisation performance on the validation set (we need to create another generator over the validation data for this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_gen = generator.flow(val_data.index, val_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create callbacks for early stopping (if validation accuracy stops improving) and best model checkpoint saving:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(\"logs\"):\n",
    "    os.makedirs(\"logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_callback = EarlyStopping(\n",
    "    monitor=\"val_acc\", patience=50\n",
    ")  # patience is the number of epochs to wait before early stopping in case of no further improvement\n",
    "\n",
    "mc_callback = ModelCheckpoint(\n",
    "    \"logs/best_ppnp_model.h5\",\n",
    "    monitor=\"val_acc\",\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = ppnp_model.fit_generator(\n",
    "    train_gen,\n",
    "    epochs=80,\n",
    "    validation_data=val_gen,\n",
    "    verbose=2,\n",
    "    shuffle=False,  # this should be False, since shuffling data means shuffling the whole graph\n",
    "    callbacks=[es_callback, mc_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the training history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def remove_prefix(text, prefix):\n",
    "    return text[text.startswith(prefix) and len(prefix):]\n",
    "\n",
    "def plot_history(history):\n",
    "    metrics = sorted(set([remove_prefix(m, \"val_\") for m in list(history.history.keys())]))\n",
    "    for m in metrics:\n",
    "        # summarize history for metric m\n",
    "        plt.plot(history.history[m])\n",
    "        plt.plot(history.history['val_' + m])\n",
    "        plt.title(m)\n",
    "        plt.ylabel(m)\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'validation'], loc='best')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reload the saved weights of the best model found during the training (according to validation accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppnp_model.load_weights(\"logs/best_ppnp_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the best model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gen = generator.flow(test_data.index, test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics = ppnp_model.evaluate_generator(test_gen)\n",
    "print(\"\\nTest Set Metrics:\")\n",
    "for name, val in zip(ppnp_model.metrics_names, test_metrics):\n",
    "    print(\"\\t{}: {:0.4f}\".format(name, val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Approximate PPNP Model\n",
    "\n",
    "Lets repeat the training and testing steps with the APPNP model using the same dataset. The downside of the PPNP is that you have to invert the adjacency matrix - which is time inneficient for large graphs - and store that invert matrix - which is space innefficient. The approximate model avoids this issue by using a clever mathematical trick. \n",
    "\n",
    "The APPNP model uses the normalized graph Laplacian. To get the normalized graph Laplacian we create a new `FullBatchNodeGenerator` and set `method=\"gcn\"`. We have the option of choosing `sparse=True` or `sparse=False` but will use `sparse=True` for memory efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = FullBatchNodeGenerator(G, method=\"gcn\", sparse=True)\n",
    "\n",
    "train_gen = generator.flow(train_data.index, train_targets)\n",
    "val_gen = generator.flow(val_data.index, val_targets)\n",
    "test_gen = generator.flow(test_data.index, test_targets)\n",
    "\n",
    "appnp = APPNP(\n",
    "    layer_sizes=[64, 64, train_targets.shape[-1]],\n",
    "    activations=[\"relu\", \"relu\", \"relu\"],\n",
    "    bias=True,\n",
    "    generator=generator,\n",
    "    teleport_probability=0.1,\n",
    "    dropout=0.5,\n",
    "    kernel_regularizer=keras.regularizers.l2(0.001),\n",
    ")\n",
    "\n",
    "x_inp, x_out = appnp.build()\n",
    "predictions = keras.layers.Softmax()(x_out)\n",
    "\n",
    "appnp_model = keras.models.Model(inputs=x_inp, outputs=predictions)\n",
    "appnp_model.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"acc\"],\n",
    "    optimizer=keras.optimizers.Adam(lr=0.01),\n",
    ")\n",
    "\n",
    "es_callback = EarlyStopping(\n",
    "    monitor=\"val_acc\", patience=50\n",
    ")  # patience is the number of epochs to wait before early stopping in case of no further improvement\n",
    "\n",
    "mc_callback = ModelCheckpoint(\n",
    "    \"logs/best_appnp_model.h5\",\n",
    "    monitor=\"val_acc\",\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    ")\n",
    "\n",
    "history = appnp_model.fit_generator(\n",
    "    train_gen,\n",
    "    epochs=120,\n",
    "    validation_data=val_gen,\n",
    "    verbose=2,\n",
    "    shuffle=False,  # this should be False, since shuffling data means shuffling the whole graph\n",
    "    callbacks=[es_callback, mc_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appnp_model.load_weights(\"logs/best_appnp_model.h5\")\n",
    "test_metrics = appnp_model.evaluate_generator(test_gen)\n",
    "print(\"\\nTest Set Metrics:\")\n",
    "for name, val in zip(appnp_model.metrics_names, test_metrics):\n",
    "    print(\"\\t{}: {:0.4f}\".format(name, val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scalable APPNP Training\n",
    "\n",
    "Now we're going to exploit the structure of PPNP for scalable training. PPNP consists of a fully-connected neural network followed by a graph propogation step. For each node, the fully-connected network outputs a score for each class and the propogaiton step basically takes a weighted average of scores of nearby nodes (closer nodes are weighted higher). \n",
    "\n",
    "Above, we trained the whole network end-to-end which obtains the most accurate results but  requires us to load the entire graph onto our GPU memory. This is because we need the entire graph for the propogation step. Unfortunately, this limits the graph size by our GPU memory. To get around this, we can train the fully-connected network separately and once we have a trained fully connected network we can add the graph propagation step.  The advantage of this approach is that we can train on batches of node features instead of the entire graph.\n",
    "\n",
    "The model in the propagation step can be any keras model trained on node features to predict the target classes. In this example we use a fully connected neural network with bag of word features as input. We could easily swap out the bag of words features for the complete text and replace the fully connected network with a state-of-the-art NLP model (for example BERT [1]), fine-tune the model and propagate its predictions.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "1. Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. https://arxiv.org/abs/1810.04805"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we create and train a fully connected model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Model()\n",
    "\n",
    "in_layer = layers.Input(shape=(len(feature_names),))\n",
    "\n",
    "layer = layers.Dropout(0.5)(in_layer)\n",
    "layer = layers.Dense(64, activation=\"relu\", kernel_regularizer=\"l2\")(layer)\n",
    "layer = layers.Dropout(0.5)(layer)\n",
    "layer = layers.Dense(64, activation=\"relu\", kernel_regularizer=\"l2\")(layer)\n",
    "layer = layers.Dropout(0.5)(layer)\n",
    "\n",
    "# note the dimension of the output should equal the number of classes to predict!\n",
    "layer = layers.Dense(train_targets.shape[-1], activation=\"relu\")(layer)\n",
    "layer = layers.Softmax()(layer)\n",
    "\n",
    "fully_connected_model = keras.models.Model(inputs=in_layer, outputs=layer)\n",
    "\n",
    "fully_connected_model.compile(\n",
    "    loss=\"categorical_crossentropy\", metrics=[\"acc\"], optimizer=optimizers.Adam(lr=0.01)\n",
    ")\n",
    "\n",
    "# the inputs are just the node features\n",
    "X_train = train_data[feature_names].values.astype(np.float32)\n",
    "X_val = val_data[feature_names].values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_callback = EarlyStopping(\n",
    "    monitor=\"val_acc\", patience=50\n",
    ")  # patience is the number of epochs to wait before early stopping in case of no further improvement\n",
    "\n",
    "mc_callback = ModelCheckpoint(\n",
    "    \"logs/best_fc_model.h5\",\n",
    "    monitor=\"val_acc\",\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    ")\n",
    "\n",
    "history = fully_connected_model.fit(\n",
    "    X_train,\n",
    "    train_targets,\n",
    "    validation_data=(X_val, val_targets),\n",
    "    epochs=2000,\n",
    "    batch_size=200,\n",
    "    shuffle=True,  # we can shuffle the data here as\n",
    "    callbacks=[es_callback, mc_callback],\n",
    ")  # we're only working with node features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By itself the fully connected model only gets ~60% accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_data[feature_names].values.astype(np.float32)\n",
    "\n",
    "fully_connected_model.load_weights(\"logs/best_fc_model.h5\")\n",
    "test_metrics = fully_connected_model.evaluate(X_test, test_targets, verbose=2)\n",
    "print(\"\\nTest Set Metrics:\")\n",
    "for name, val in zip(fully_connected_model.metrics_names, test_metrics):\n",
    "    print(\"\\t{}: {:0.4f}\".format(name, val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we propogate the fully connected network - no extra training required and we can re-use the APPNP object we've already created. First we create an intermediate fully connected model without the softmax layer, this is to avoid propagating the softmax layer which may cause issues with further training. We then propagate this intermediate network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_model = Model(\n",
    "    inputs=fully_connected_model.inputs, outputs=fully_connected_model.layers[-2].output\n",
    ")\n",
    "\n",
    "x_inp, x_out = appnp.propagate_model(intermediate_model)\n",
    "predictions = keras.layers.Softmax()(x_out)\n",
    "\n",
    "propagated_model = keras.models.Model(inputs=x_inp, outputs=predictions)\n",
    "propagated_model.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"acc\"],\n",
    "    optimizer=keras.optimizers.Adam(lr=0.01),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our accuracy is better than the fully connected network by itself but less than end-to-end trained PPNP and APPNP. \n",
    "\n",
    "Note that this is partially because 140 data points isn't sufficient for the fully connected model to achieve optimal performance. As the number of training nodes increases the perfomance gap shrinks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics = propagated_model.evaluate_generator(test_gen)\n",
    "print(\"\\nTest Set Metrics:\")\n",
    "for name, val in zip(propagated_model.metrics_names, test_metrics):\n",
    "    print(\"\\t{}: {:0.4f}\".format(name, val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making predictions with the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get the predictions for all nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nodes = node_data.index\n",
    "all_gen = generator.flow(all_nodes)\n",
    "all_predictions = propagated_model.predict_generator(all_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These predictions will be the output of the softmax layer, so to get final categories we'll use the `inverse_transform` method of our target attribute specifcation to turn these values back to the original categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that for full-batch methods the batch size is 1 and the predictions have shape $(1, N_{nodes}, N_{classes})$ so we remove the batch dimension to obtain predictions of shape $(N_{nodes}, N_{classes})$ using the NumPy `squeeze` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_predictions = target_encoding.inverse_transform(all_predictions.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at a few predictions after training the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(node_predictions, index=all_nodes).idxmax(axis=1)\n",
    "df = pd.DataFrame({\"Predicted\": results, \"True\": node_data[\"subject\"]})\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have an accurate model that can handle large graphs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
