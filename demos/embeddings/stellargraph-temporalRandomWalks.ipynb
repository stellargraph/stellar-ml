{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous-Time Dynamic Network Embeddings\n",
    "\n",
    "This is the demo of [Continuous-Time Dynamic Network Embeddings](http://ryanrossi.com/pubs/nguyen-et-al-WWW18-BigNet.pdf) implemented in Stellargraph GraphML library. This demo is to show in simple steps how time respecting walks can be done on a temporal graph and how they can be used to get network embeddings. \n",
    "\n",
    "We compare the embeddings learnt from temporal walks with non-temporal walks in this demo. \n",
    "\n",
    "## Data set\n",
    "\n",
    "We use the **Social Spammers in Evolving Multi-Relational Social Network Dataset** to demonstrate the continuous-time dynamic network embeddings method. This dataset was released by Shobeir Fakhraei with permission from if(we) Inc. as supplementary material of the paper: [Collective Spammer Detection in Evolving Multi-Relational Social Networks](http://www.cs.umd.edu/~shobeir/papers/fakhraei_kdd_2015.pdf). \n",
    "The original anonymized dataset was collected from the Tagged.com social network website.\n",
    "It contains 5.6 million users and 858 million links between them.\n",
    "Each user has 4 features and is manually labeled as \"spammer\" or \"not spammer\".\n",
    "Each link represents an action between two users and includes a timestamp and a type.\n",
    "The network contains 7 anonymized types of links. The original task on the dataset is to identify (i.e., classify) the spammer users based on their relational and non-relational features.\n",
    "https://linqs-data.soe.ucsc.edu/public/social_spammer/\n",
    "\n",
    "<a name=\"refs\"></a>\n",
    "**References**\n",
    "\n",
    "[1] Continuous-Time Dynamic Network Embeddings. Giang Hoang Nguyen, John Boaz Lee, Ryan A. Rossi, Nesreen K. Ahmed. ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD), 2016. ([link](http://ryanrossi.com/pubs/nguyen-et-al-WWW18-BigNet.pdf))\n",
    "\n",
    "[2] Collective Spammer Detection in Evolving Multi-Relational Social Networks.Fakhraei, Shobeir and Foulds, James and Shashanka, Madhusudana and Getoor, Lise. Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. KDD 2015. ([link](http://www.cs.umd.edu/~shobeir/papers/fakhraei_kdd_2015.pdf))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Details of the method:\n",
    "The overall approach of temporal random walks is  quite simple. \n",
    "\n",
    "Starting from a node randomly pick each the next node forward in time. \n",
    "\n",
    "**Choices made in the paper:**\n",
    " * Initial temporal edge selection:  *where to start the walk?*\n",
    "    * Unbiased:\n",
    "        * Uniformly at random select a starting edge.\n",
    "    * Biased:\n",
    "        * Exponential\n",
    "        * Linear\n",
    "\n",
    " * Temporal random walks: *how to select the next edge?* \n",
    "    * Unbiased:\n",
    "       \n",
    "       Uniformly at random select the next edge that has a timestamp greater than the current timestamp and starts at the destination node of the current edge.\n",
    "    * Biased: \n",
    "        * Exponential \n",
    "        * Linear\n",
    "\n",
    "* Arbitrary length of walks:\n",
    "    \n",
    "    Unlike timeless random walks, there may not be enough options to perform a time-based random walk of a certain fixed length L. A minimum value \\omega is set for a minimum viable walk. All walks of length greater than and equal to omega and less than and equal to L are considered as valid walks to extract the (target, context ) pairs.\n",
    "\n",
    "\n",
    "\n",
    "**Stellargraph strategies:**\n",
    "* Initial temporal edge selection:  *where to start the walk?* \n",
    "    * Slightly departing from the paperâ€™s idea, given a set of head nodes, uniformly at random select a starting edge.\n",
    "        \n",
    "* Temporal random walks: *how to select the next edge?* \n",
    "    * Uniformly at random\n",
    "    * Exponentially decaying in time\n",
    "* Bidirectional walks: the walker can perform forward or bidirectional *time respecting* random walks continuting from the latest or both outer end nodes of the random walk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import networkx as nx\n",
    "import random\n",
    "from numpy.random import choice\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn import preprocessing, feature_extraction, model_selection\n",
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spammers dataset\n",
    "The spammers dataset can be downloaded from here: https://linqs-data.soe.ucsc.edu/public/social_spammer/. It has two components: user data and the relationships data. For efficiency, in this demo, we utilize the userID and label (spammers = 1, non-spammers = 0) from the User data. From the relationships data, we use only day 0 and relation 1 as the extant dataset is huge.\n",
    "\n",
    "You can update `data_path` to the location of your downloaded dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"~/data/social-spammer/\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**User data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>gender</th>\n",
       "      <th>timePassedValidation</th>\n",
       "      <th>ageGroup</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>M</td>\n",
       "      <td>0.9000</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>F</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>M</td>\n",
       "      <td>0.1375</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>M</td>\n",
       "      <td>0.3875</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>M</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  userID gender  timePassedValidation  ageGroup  label\n",
       "0      1      M                0.9000        30      0\n",
       "1      2      F                1.0000        20      0\n",
       "2      3      M                0.1375        30      0\n",
       "3      4      M                0.3875        20      0\n",
       "4      5      M                0.0125        20      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_users = pd.read_csv(data_path + 'usersdata.csv', sep='\\t', header=None)\n",
    "df_users.columns = ['userID', 'gender', 'timePassedValidation','ageGroup', 'label']\n",
    "df_users['userID'] = df_users['userID'].astype('str')\n",
    "df_users.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Relationships**\n",
    "Loading the relationships between the users (emails from source to target). For this demo using only 1 day (day = 0) of data and one relationship type (relations = 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_relations = pd.read_csv(data_path + 'relations.csv', sep ='\\t', header=None)\n",
    "df_relations.columns = ['day', 'time_ms', 'source', 'target', 'relations']\n",
    "df_relations_day0 =  df_relations[df_relations['day'] == 0]\n",
    "df_relations_day0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_relations_day0.time_ms.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_relations_day0.source.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_relations_day0.target.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_relations_day0.relations.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_relations_day0_rel1 = df_relations_day0[df_relations_day0['relations'] == 1]\n",
    "df_relations_day0_rel1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_relations_day0_rel1['source'] = df_relations_day0_rel1['source'].astype('str')\n",
    "df_relations_day0_rel1['target'] = df_relations_day0_rel1['target'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create lit of unique users from source and target\n",
    "u1 = df_relations_day0_rel1.iloc[:,2]\n",
    "u2 = df_relations_day0_rel1.iloc[:,3]\n",
    "users_day_0 = pd.concat([u1, u2], ignore_index=True)\n",
    "users_day_0 = pd.Series.unique(users_day_0)\n",
    "print(\"In the network there are {} users out of total {} users.\".format(len(users_day_0), len(df_users)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Distribution of spammers and non-spammers labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spammer_dist = df_users[df_users['userID'].isin(users_day_0)].label.value_counts()\n",
    "print(\"The distribution of spammers in our network is\\n {}\".format(spammer_dist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Only 7.7 % spammers.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_relations_day0_rel1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GDay0= nx.from_pandas_edgelist(df_relations_day0_rel1, 'source', 'target', edge_attr=['time_ms'], \n",
    "                                 create_using=nx.MultiDiGraph())\n",
    "print(nx.info(GDay0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_attr = df_users[df_users['userID'].isin(users_day_0)]\n",
    "values = { str(row.tolist()[0]): row.tolist()[-1] for _, row in node_attr.iterrows() }\n",
    "nx.set_node_attributes(GDay0, values, 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = [(node,val) for (node, val) in GDay0.degree()]\n",
    "df = pd.DataFrame(degrees, columns=['userID', 'degree'])\n",
    "df_degrees_labels = pd.merge(df, df_users[['userID', 'label']], on='userID')\n",
    "df_degrees_labels.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_degrees_labels.groupby(['label']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_degrees_labels[df_degrees_labels['label']==0].hist(column = ['degree'], log = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_degrees_labels[df_degrees_labels['label']==1].hist(column = ['degree'],log = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random walks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stellargraph import StellarGraph, StellarDiGraph\n",
    "from stellargraph.data import BiasedRandomWalk\n",
    "from stellargraph.data import TemporalUniformRandomWalk\n",
    "from stellargraph.data import TemporalBiasedRandomWalk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a Stellargraph object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_weight_label = \"time_ms\"\n",
    "g = StellarGraph(GDay0, edge_weight_label=edge_weight_label)\n",
    "print(g.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set parameters for the walks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = list(GDay0.nodes)\n",
    "n = 5\n",
    "length = 20\n",
    "bidirectional = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Biased Random Walks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rw = BiasedRandomWalk(g)\n",
    "\n",
    "walks = rw.run(\n",
    "    nodes=nodes, # root nodes\n",
    "    length=length,  # maximum length of a random walk\n",
    "    n=n,        # number of random walks per root node \n",
    "    p=0.5,       # Defines (unormalised) probability, 1/p, of returning to source node\n",
    "    q=2.0        # Defines (unormalised) probability, 1/q, for moving away from source node\n",
    ")\n",
    "print(\"Number of random walks: {}\".format(len(walks)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Temporal Random Walks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trw = TemporalRandomWalk(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_random_walks = trw.run(\n",
    "    nodes=nodes, # root nodes\n",
    "    length=length,  # maximum length of a random walk\n",
    "    n=n,\n",
    "    bidirectional=bidirectional\n",
    "    step_type=\"uniform\"\n",
    ")\n",
    "print(\"Number of random walks: {}\".format(len(temporal_random_walks)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_biased_random_walks = trw.run(\n",
    "    nodes=nodes, # root nodes\n",
    "    length=length,  # maximum length of a random walk\n",
    "    n=n,\n",
    "    bidirectional=bidirectional\n",
    "    step_type=\"exponential\"\n",
    ")\n",
    "print(\"Number of random walks: {}\".format(len(temporal_biased_random_walks)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representation Learning using Word2Vec\n",
    "\n",
    "We use the Word2Vec [[2]](#refs) implementation in the free Python library gensim [[3]](#refs), to learn representations for each node in the graph.\n",
    "\n",
    "We set the dimensionality of the learned embedding vectors to 128 as in [[1]](#refs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(walks, size=128, window=5, min_count=0, sg=1, workers=2, iter=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_temporal_uniform = Word2Vec(temporal_random_walks, size=128, window=5, min_count=0, sg=1, workers=2, iter=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_temporal_biased = Word2Vec(temporal_biased_random_walks, size=128, window=5, min_count=0, sg=1, workers=2, iter=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downstream task\n",
    "\n",
    "The node embeddings calculated using the biased (non-temporal) and temporal walks can be used as node feature vectors in a downstream task such as node classification. \n",
    "\n",
    "In this example, we will use the  node embeddings to train a simple Logistic Regression classifier to predict spammers and non-spammers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Biased Random walks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve node embeddings and corresponding subjects\n",
    "node_ids = model.wv.index2word  # list of node IDs\n",
    "node_embeddings = model.wv.vectors  # numpy.ndarray of size number of nodes times embeddings dimensionality\n",
    "node_targets = [ GDay0.node[node_id]['label'] for node_id in node_ids ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X will hold the 50 input features (node embeddings)\n",
    "X = node_embeddings  \n",
    "# y holds the corresponding target values\n",
    "y = np.array(node_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Splitting**\n",
    "\n",
    "We split the data into train and test sets. \n",
    "\n",
    "We use 5% of the data for training and the remaining 95% for testing as a hold out test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.05, test_size=None, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "print(collections.Counter(y))\n",
    "print(collections.Counter(y_train))\n",
    "print(collections.Counter(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifier Training\n",
    "\n",
    "We train a Logistic Regression classifier on the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "class_weights = compute_class_weight('balanced', \n",
    "                                     np.unique(y_train), \n",
    "                                     y_train)\n",
    "train_class_weights = dict(zip(np.unique(y_train), \n",
    "                               class_weights))\n",
    "train_class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(verbose=0, solver='lbfgs', multi_class=\"auto\", class_weight=train_class_weights)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the hold out test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the accuracy of the classifier on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predicted classes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(y_pred).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**True classes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.Series(y).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Uniform temporal Random walks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve node embeddings and corresponding subjects\n",
    "node_ids = model_temporal_uniform.wv.index2word  # list of node IDs\n",
    "node_embeddings = model_temporal_uniform.wv.vectors  # numpy.ndarray of size number of nodes times embeddings dimensionality\n",
    "node_targets = [ GDay0.node[node_id]['label'] for node_id in node_ids ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X will hold the 50 input features (node embeddings)\n",
    "X = node_embeddings  \n",
    "# y holds the corresponding target values\n",
    "y = np.array(node_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Splitting**\n",
    "\n",
    "We split the data into train and test sets. \n",
    "\n",
    "We use 5% of the data for training and the remaining 95% for testing as a hold out test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.05, test_size=None, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifier Training\n",
    "\n",
    "We train a Logistic Regression classifier on the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(verbose=0, solver='lbfgs', multi_class=\"auto\", class_weight= 'balanced')\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the hold out test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the accuracy of the classifier on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predicted classes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(y_pred).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**True classes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.Series(y).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Biased temporal Random walks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve node embeddings and corresponding subjects\n",
    "node_ids = model_temporal_biased.wv.index2word  # list of node IDs\n",
    "node_embeddings = model_temporal_biased.wv.vectors  # numpy.ndarray of size number of nodes times embeddings dimensionality\n",
    "node_targets = [ GDay0.node[node_id]['label'] for node_id in node_ids ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X will hold the 50 input features (node embeddings)\n",
    "X = node_embeddings  \n",
    "# y holds the corresponding target values\n",
    "y = np.array(node_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Splitting**\n",
    "\n",
    "We split the data into train and test sets. \n",
    "\n",
    "We use 5% of the data for training and the remaining 95% for testing as a hold out test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.05, test_size=None, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifier Training\n",
    "\n",
    "We train a Logistic Regression classifier on the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(verbose=0, solver='lbfgs', multi_class=\"auto\", class_weight='balanced')\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the hold out test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the accuracy of the classifier on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predicted classes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(y_pred).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**True classes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.Series(y).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise Node Embeddings\n",
    "\n",
    "We retrieve the Word2Vec node embeddings that are 128-dimensional vectors and then we project them down to 2 dimensions using PCA algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Biased Random walks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve node embeddings and corresponding subjects\n",
    "node_ids = model.wv.index2word  # list of node IDs\n",
    "node_embeddings = model.wv.vectors  # numpy.ndarray of size number of nodes times embeddings dimensionality\n",
    "node_targets = [ GDay0.node[node_id]['label'] for node_id in node_ids ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the embeddings to 2d space for visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = PCA #TSNE\n",
    "\n",
    "trans = transform(n_components=2)\n",
    "node_embeddings_2d = trans.fit_transform(node_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw the embedding points, coloring them by the target label (paper subject)\n",
    "alpha = 0.7\n",
    "label_map = { l: i for i, l in enumerate(np.unique(node_targets)) }\n",
    "node_colours = [ label_map[target] for target in node_targets ]\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.axes().set(aspect=\"equal\")\n",
    "plt.scatter(node_embeddings_2d[:,0], \n",
    "            node_embeddings_2d[:,1], \n",
    "            c=node_colours, cmap=\"jet\", alpha=alpha)\n",
    "plt.title('{} visualization of node embeddings'.format(transform.__name__))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal Uniform Random walks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve node embeddings and corresponding subjects\n",
    "node_ids = model_temporal_uniform.wv.index2word  # list of node IDs\n",
    "node_embeddings = model_temporal_uniform.wv.vectors  # numpy.ndarray of size number of nodes times embeddings dimensionality\n",
    "node_targets = [ GDay0.node[node_id]['label'] for node_id in node_ids ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the embeddings to 2d space for visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = PCA #TSNE\n",
    "\n",
    "trans = transform(n_components=2)\n",
    "node_embeddings_2d = trans.fit_transform(node_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw the embedding points, coloring them by the target label (paper subject)\n",
    "alpha = 0.7\n",
    "label_map = { l: i for i, l in enumerate(np.unique(node_targets)) }\n",
    "node_colours = [ label_map[target] for target in node_targets ]\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.axes().set(aspect=\"equal\")\n",
    "plt.scatter(node_embeddings_2d[:,0], \n",
    "            node_embeddings_2d[:,1], \n",
    "            c=node_colours, cmap=\"jet\", alpha=alpha)\n",
    "plt.title('{} visualization of node embeddings'.format(transform.__name__))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal Biased Random walks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve node embeddings and corresponding subjects\n",
    "node_ids = model_temporal_biased.wv.index2word  # list of node IDs\n",
    "node_embeddings = model_temporal_biased.wv.vectors  # numpy.ndarray of size number of nodes times embeddings dimensionality\n",
    "node_targets = [ GDay0.node[node_id]['label'] for node_id in node_ids ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the embeddings to 2d space for visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = PCA #TSNE\n",
    "\n",
    "trans = transform(n_components=2)\n",
    "node_embeddings_2d = trans.fit_transform(node_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw the embedding points, coloring them by the target label (paper subject)\n",
    "alpha = 0.7\n",
    "label_map = { l: i for i, l in enumerate(np.unique(node_targets)) }\n",
    "node_colours = [ label_map[target] for target in node_targets ]\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.axes().set(aspect=\"equal\")\n",
    "plt.scatter(node_embeddings_2d[:,0], \n",
    "            node_embeddings_2d[:,1], \n",
    "            c=node_colours, cmap=\"jet\", alpha=alpha)\n",
    "plt.title('{} visualization of node embeddings'.format(transform.__name__))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
