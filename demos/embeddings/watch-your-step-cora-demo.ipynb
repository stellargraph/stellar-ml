{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stellargraph.core import StellarGraph\n",
    "from stellargraph.mapper import AdjacencyPowerGenerator\n",
    "from stellargraph.layer.watch_your_step import WatchYourStep, get_embeddings\n",
    "from stellargraph.losses import graph_log_likelihood\n",
    "from stellargraph import datasets\n",
    "from stellargraph.utils import plot_history\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras import optimizers, Model, layers, regularizers\n",
    "import tensorflow as tf\n",
    "from sklearn import preprocessing, feature_extraction, model_selection\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import networkx as nx\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "The Cora dataset consists of 2708 scientific publications classified into one of seven classes. The citation network consists of 5429 links. Each publication in the dataset is described by a 0/1-valued word vector indicating the absence/presence of the corresponding word from the dictionary. The dictionary consists of 1433 unique words."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = datasets.Cora()\n",
    "display(HTML(dataset.description))\n",
    "G, subjects = dataset.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the model\n",
    "\n",
    "We create an `AdjacencyPowerGenerator` which loops through the rows of the first `num_powers` of the adjacency matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = AdjacencyPowerGenerator(G, num_powers=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use the `WatchYourStep` class to create trainable node embeddings and expected random walks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "wys = WatchYourStep(\n",
    "    generator,\n",
    "    num_walks=80,\n",
    "    embedding_dimension=128,\n",
    "    attention_regularizer=regularizers.l2(0.5),\n",
    ")\n",
    "x_in, x_out = wys.build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the graph log likelihood as our loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=x_in, outputs=x_out)\n",
    "model.compile(loss=graph_log_likelihood, optimizer=tf.keras.optimizers.Adam(1e-3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "We now create a training generator and fit our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 270 steps\n",
      "Epoch 1/100\n",
      "270/270 [==============================] - 2s 6ms/step - loss: 186700.2018\n",
      "Epoch 2/100\n",
      "270/270 [==============================] - 1s 5ms/step - loss: 178969.1792\n",
      "Epoch 3/100\n",
      "270/270 [==============================] - 1s 5ms/step - loss: 129431.9966\n",
      "Epoch 4/100\n",
      "270/270 [==============================] - 1s 5ms/step - loss: 57934.4538\n",
      "Epoch 5/100\n",
      "270/270 [==============================] - 1s 5ms/step - loss: 32088.0088\n",
      "Epoch 6/100\n",
      "270/270 [==============================] - 1s 5ms/step - loss: 28159.3023\n",
      "Epoch 7/100\n",
      "270/270 [==============================] - 1s 5ms/step - loss: 27210.7526\n",
      "Epoch 8/100\n",
      "270/270 [==============================] - 1s 5ms/step - loss: 26804.0880\n",
      "Epoch 9/100\n",
      "270/270 [==============================] - 1s 5ms/step - loss: 26550.2599\n",
      "Epoch 10/100\n",
      "270/270 [==============================] - 1s 5ms/step - loss: 26333.4867\n",
      "Epoch 11/100\n",
      "270/270 [==============================] - 1s 6ms/step - loss: 26084.3134\n",
      "Epoch 12/100\n",
      "270/270 [==============================] - 1s 5ms/step - loss: 25774.0277\n",
      "Epoch 13/100\n",
      "270/270 [==============================] - 1s 5ms/step - loss: 25299.3481\n",
      "Epoch 14/100\n",
      "270/270 [==============================] - 1s 4ms/step - loss: 24528.7632\n",
      "Epoch 15/100\n",
      "270/270 [==============================] - 1s 4ms/step - loss: 23338.1738\n",
      "Epoch 16/100\n",
      "270/270 [==============================] - 1s 4ms/step - loss: 21667.0645\n",
      "Epoch 17/100\n",
      "270/270 [==============================] - 1s 4ms/step - loss: 19908.9744\n",
      "Epoch 18/100\n",
      "270/270 [==============================] - 1s 4ms/step - loss: 18566.8842\n",
      "Epoch 19/100\n",
      "270/270 [==============================] - 1s 5ms/step - loss: 17358.8326\n",
      "Epoch 20/100\n",
      "270/270 [==============================] - 1s 5ms/step - loss: 16459.8890\n",
      "Epoch 21/100\n",
      "270/270 [==============================] - 1s 5ms/step - loss: 15328.4564\n",
      "Epoch 22/100\n",
      "270/270 [==============================] - 1s 4ms/step - loss: 14063.0619\n",
      "Epoch 23/100\n",
      "270/270 [==============================] - 1s 4ms/step - loss: 12876.2096\n",
      "Epoch 24/100\n",
      "270/270 [==============================] - 1s 4ms/step - loss: 11908.1559\n",
      "Epoch 25/100\n",
      "270/270 [==============================] - 1s 4ms/step - loss: 10964.3430\n",
      "Epoch 26/100\n",
      "270/270 [==============================] - 1s 4ms/step - loss: 10162.7132\n",
      "Epoch 27/100\n",
      "270/270 [==============================] - 1s 4ms/step - loss: 9637.1716\n",
      "Epoch 28/100\n",
      "270/270 [==============================] - 1s 4ms/step - loss: 9263.5900\n",
      "Epoch 29/100\n",
      "270/270 [==============================] - 1s 5ms/step - loss: 8759.2297\n",
      "Epoch 30/100\n",
      "270/270 [==============================] - 1s 5ms/step - loss: 8373.2793\n",
      "Epoch 31/100\n",
      "270/270 [==============================] - 1s 5ms/step - loss: 8050.3759\n",
      "Epoch 32/100\n",
      "270/270 [==============================] - 1s 5ms/step - loss: 7708.6396\n",
      "Epoch 33/100\n",
      "270/270 [==============================] - 1s 5ms/step - loss: 7400.0407\n",
      "Epoch 34/100\n",
      "270/270 [==============================] - 1s 5ms/step - loss: 7214.9141\n",
      "Epoch 35/100\n",
      "270/270 [==============================] - 1s 5ms/step - loss: 7014.3548\n",
      "Epoch 36/100\n",
      "270/270 [==============================] - 1s 5ms/step - loss: 6820.5909\n",
      "Epoch 37/100\n",
      "270/270 [==============================] - 1s 5ms/step - loss: 6594.1424\n",
      "Epoch 38/100\n",
      "270/270 [==============================] - 1s 5ms/step - loss: 6406.8553\n",
      "Epoch 39/100\n",
      "270/270 [==============================] - 1s 4ms/step - loss: 6321.7023\n",
      "Epoch 40/100\n",
      "270/270 [==============================] - 2s 6ms/step - loss: 6201.9015\n",
      "Epoch 41/100\n",
      "270/270 [==============================] - 2s 6ms/step - loss: 6028.7581\n",
      "Epoch 42/100\n",
      "270/270 [==============================] - 2s 6ms/step - loss: 5895.0538\n",
      "Epoch 43/100\n",
      "270/270 [==============================] - 1s 6ms/step - loss: 5835.7105\n",
      "Epoch 44/100\n",
      "270/270 [==============================] - 1s 5ms/step - loss: 5724.8243\n",
      "Epoch 45/100\n",
      "270/270 [==============================] - 1s 5ms/step - loss: 5627.1006\n",
      "Epoch 46/100\n",
      "270/270 [==============================] - 1s 5ms/step - loss: 5480.6956\n",
      "Epoch 47/100\n",
      "270/270 [==============================] - 1s 5ms/step - loss: 5452.4856\n",
      "Epoch 48/100\n",
      "270/270 [==============================] - 1s 5ms/step - loss: 5365.7522\n",
      "Epoch 49/100\n",
      "270/270 [==============================] - 2s 6ms/step - loss: 5287.0739\n",
      "Epoch 50/100\n",
      "270/270 [==============================] - 1s 5ms/step - loss: 5172.1534\n",
      "Epoch 51/100\n",
      "270/270 [==============================] - 1s 5ms/step - loss: 5099.3168\n",
      "Epoch 52/100\n",
      "270/270 [==============================] - 1s 5ms/step - loss: 5054.8099\n",
      "Epoch 53/100\n",
      "270/270 [==============================] - 1s 5ms/step - loss: 5013.4648\n",
      "Epoch 54/100\n",
      "270/270 [==============================] - 1s 5ms/step - loss: 4891.9778\n",
      "Epoch 55/100\n",
      "270/270 [==============================] - 1s 5ms/step - loss: 4843.2927\n",
      "Epoch 56/100\n",
      "270/270 [==============================] - 1s 5ms/step - loss: 4764.0629\n",
      "Epoch 57/100\n",
      "270/270 [==============================] - 1s 5ms/step - loss: 4759.7702\n",
      "Epoch 58/100\n",
      "270/270 [==============================] - 1s 5ms/step - loss: 4643.5637\n",
      "Epoch 59/100\n",
      "270/270 [==============================] - 1s 5ms/step - loss: 4603.7698\n",
      "Epoch 60/100\n",
      "270/270 [==============================] - 1s 5ms/step - loss: 4525.8638\n",
      "Epoch 61/100\n",
      "270/270 [==============================] - 1s 5ms/step - loss: 4494.2021\n",
      "Epoch 62/100\n",
      "270/270 [==============================] - 1s 5ms/step - loss: 4416.9934\n",
      "Epoch 63/100\n",
      "270/270 [==============================] - 1s 5ms/step - loss: 4393.4224\n",
      "Epoch 64/100\n",
      "270/270 [==============================] - 1s 5ms/step - loss: 4296.0524\n",
      "Epoch 65/100\n",
      "270/270 [==============================] - 1s 5ms/step - loss: 4243.8265\n",
      "Epoch 66/100\n",
      "270/270 [==============================] - 1s 6ms/step - loss: 4196.4405\n",
      "Epoch 67/100\n",
      "270/270 [==============================] - 1s 5ms/step - loss: 4198.3916\n",
      "Epoch 68/100\n",
      "270/270 [==============================] - 1s 5ms/step - loss: 4105.5322\n",
      "Epoch 69/100\n",
      "270/270 [==============================] - 1s 4ms/step - loss: 4023.5229\n",
      "Epoch 70/100\n",
      "270/270 [==============================] - 1s 5ms/step - loss: 3965.5881\n",
      "Epoch 71/100\n",
      "270/270 [==============================] - 1s 5ms/step - loss: 3992.0402\n",
      "Epoch 72/100\n",
      "270/270 [==============================] - 1s 5ms/step - loss: 3924.8584\n",
      "Epoch 73/100\n",
      "270/270 [==============================] - 1s 5ms/step - loss: 3854.4957\n",
      "Epoch 74/100\n",
      "270/270 [==============================] - 1s 5ms/step - loss: 3747.1157\n",
      "Epoch 75/100\n",
      "270/270 [==============================] - 1s 5ms/step - loss: 3746.8883\n",
      "Epoch 76/100\n",
      "270/270 [==============================] - 1s 5ms/step - loss: 3745.7367\n",
      "Epoch 77/100\n",
      "270/270 [==============================] - 1s 5ms/step - loss: 3707.4431\n",
      "Epoch 78/100\n",
      "270/270 [==============================] - 1s 4ms/step - loss: 3560.1033\n",
      "Epoch 79/100\n",
      "270/270 [==============================] - 1s 5ms/step - loss: 3535.2082\n",
      "Epoch 80/100\n",
      "270/270 [==============================] - 1s 5ms/step - loss: 3520.7062\n",
      "Epoch 81/100\n",
      "270/270 [==============================] - 1s 5ms/step - loss: 3517.3327\n",
      "Epoch 82/100\n",
      "270/270 [==============================] - 1s 5ms/step - loss: 3411.2777\n",
      "Epoch 83/100\n",
      "270/270 [==============================] - 1s 5ms/step - loss: 3340.9007\n",
      "Epoch 84/100\n",
      "270/270 [==============================] - 1s 5ms/step - loss: 3311.1953\n",
      "Epoch 85/100\n",
      "270/270 [==============================] - 1s 5ms/step - loss: 3326.1189\n",
      "Epoch 86/100\n",
      "270/270 [==============================] - 1s 5ms/step - loss: 3273.9396\n",
      "Epoch 87/100\n",
      "270/270 [==============================] - 1s 5ms/step - loss: 3179.6683\n",
      "Epoch 88/100\n",
      "270/270 [==============================] - 1s 5ms/step - loss: 3108.0218\n",
      "Epoch 89/100\n",
      "270/270 [==============================] - 1s 5ms/step - loss: 3095.9183\n",
      "Epoch 90/100\n",
      "270/270 [==============================] - 1s 5ms/step - loss: 3087.0323\n",
      "Epoch 91/100\n",
      "230/270 [========================>.....] - ETA: 0s - loss: 3046.5226"
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "train_gen = generator.flow(batch_size=batch_size, threads=10)\n",
    "\n",
    "history = model.fit(\n",
    "    train_gen, epochs=epochs, verbose=1, steps_per_epoch=int(len(G.nodes()) // batch_size)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Embeddings\n",
    "\n",
    "Now we use TSNE to visualize the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = get_embeddings(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "nodelist = list(G.nodes())\n",
    "\n",
    "labels = subjects.loc[nodelist]\n",
    "target_encoding = OneHotEncoder(sparse=False)\n",
    "label_vectors = target_encoding.fit_transform(labels.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = TSNE\n",
    "\n",
    "trans = transform(n_components=2)\n",
    "emb_transformed = pd.DataFrame(trans.fit_transform(embeddings), index=nodelist)\n",
    "\n",
    "emb_transformed[\"label\"] = np.argmax(label_vectors, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.7\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "ax.scatter(\n",
    "    emb_transformed[0],\n",
    "    emb_transformed[1],\n",
    "    c=emb_transformed[\"label\"].astype(\"category\"),\n",
    "    cmap=\"jet\",\n",
    "    alpha=alpha,\n",
    ")\n",
    "\n",
    "ax.set(aspect=\"equal\", xlabel=\"$X_1$\", ylabel=\"$X_2$\")\n",
    "plt.title(\n",
    "    \"{} visualization of Watch Your Step embeddings for cora dataset\".format(\n",
    "        transform.__name__\n",
    "    )\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "Here, we predict the class of a node by performing a weighted average of the training labels, with the weights determined by the similarity of that node's embedding with the training node embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose a random set of training nodes by permuting the labels and taking the first 300.\n",
    "shuffled_idx = np.random.permutation(label_vectors.shape[0])\n",
    "train_node_idx = shuffled_idx[:300]\n",
    "test_node_idx = shuffled_idx[300:]\n",
    "\n",
    "training_labels = label_vectors.copy()\n",
    "training_labels[test_node_idx] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = embeddings.shape[1] // 2\n",
    "\n",
    "predictions = np.dot(\n",
    "    np.exp(np.dot(embeddings[:, :d], embeddings[:, d:].transpose())), training_labels\n",
    ")\n",
    "\n",
    "np.mean(\n",
    "    np.argmax(predictions[test_node_idx], 1) == np.argmax(label_vectors[test_node_idx], 1)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
