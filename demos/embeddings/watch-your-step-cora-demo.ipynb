{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stellargraph.core import StellarGraph\n",
    "from stellargraph.mapper import AdjacencyPowerGenerator\n",
    "from stellargraph.layer.watch_your_step import WatchYourStep\n",
    "from stellargraph.utils.loss import graph_log_likelihood\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras import optimizers, Model, layers, regularizers\n",
    "from sklearn import preprocessing, feature_extraction, model_selection\n",
    "\n",
    "import networkx as nx\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.expanduser(\"~/data/cora\")\n",
    "os.listdir(data_dir)\n",
    "edgelist = pd.read_csv(os.path.join(data_dir, \"cora.cites\"), sep='\\t', header=None, names=[\"target\", \"source\"])\n",
    "edgelist[\"label\"] = \"cites\"\n",
    "\n",
    "Gnx = nx.from_pandas_edgelist(edgelist, edge_attr=\"label\")\n",
    "nx.set_node_attributes(Gnx, \"paper\", \"label\")\n",
    "\n",
    "feature_names = [\"w_{}\".format(ii) for ii in range(1433)]\n",
    "column_names =  feature_names + [\"subject\"]\n",
    "node_data = pd.read_csv(os.path.join(data_dir, \"cora.content\"), sep='\\t', header=None, names=column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the model\n",
    "\n",
    "First, we create a StellarGraph object from our NetworkX graph. From this, we create an `AdjacencyPowerGenerator` which loops through the rows of the first `num_powers` of the adjacency matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kieranricardo/PycharmProjects/origin-stellargaph/stellargraph/stellargraph/core/experimental.py:88: ExperimentalWarning: AdjacencyPowerGenerator is experimental: lack of unit tests. It may be difficult to use and may have major changes at any time.\n",
      "  warnings.warn(direct_msg, ExperimentalWarning)\n"
     ]
    }
   ],
   "source": [
    "G = StellarGraph(Gnx)\n",
    "generator = AdjacencyPowerGenerator(G, num_powers=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use the `WatchYourStep` class to create trainable node embeddings and expected random walks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kieranricardo/PycharmProjects/origin-stellargaph/stellargraph/stellargraph/core/experimental.py:88: ExperimentalWarning: WatchYourStep is experimental: lack of unit tests. It may be difficult to use and may have major changes at any time.\n",
      "  warnings.warn(direct_msg, ExperimentalWarning)\n",
      "/Users/kieranricardo/PycharmProjects/origin-stellargaph/stellargraph/stellargraph/core/experimental.py:88: ExperimentalWarning: AttentiveWalk is experimental: lack of unit tests. It may be difficult to use and may have major changes at any time.\n",
      "  warnings.warn(direct_msg, ExperimentalWarning)\n"
     ]
    }
   ],
   "source": [
    "wys = WatchYourStep(\n",
    "    generator, num_walks=80, \n",
    "    embedding_dimension=65, \n",
    "    attention_regularizer=regularizers.l2(0.5),\n",
    ")\n",
    "x_in, x_out = wys.build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the graph log likelihood as our loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kieranricardo/PycharmProjects/origin-stellargaph/stellargraph/stellargraph/core/experimental.py:88: ExperimentalWarning: graph_log_likelihood is experimental: lack of unit tests. It may be difficult to use and may have major changes at any time.\n",
      "  warnings.warn(direct_msg, ExperimentalWarning)\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=x_in, outputs=x_out)\n",
    "model.compile(loss=graph_log_likelihood, optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "We now create a training generator and fit our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kieranricardo/anaconda3/envs/tf2-stellar/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/api.py:396: ExperimentalWarning: partial_powers is experimental: lack of unit tests. It may be difficult to use and may have major changes at any time.\n",
      "  return py_builtins.overload_of(f)(*args)\n",
      "/Users/kieranricardo/anaconda3/envs/tf2-stellar/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/api.py:396: ExperimentalWarning: select_row_from_sparse_tensor is experimental: lack of unit tests. It may be difficult to use and may have major changes at any time.\n",
      "  return py_builtins.overload_of(f)(*args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 270 steps\n",
      "Epoch 1/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kieranricardo/PycharmProjects/origin-stellargaph/stellargraph/stellargraph/core/experimental.py:88: ExperimentalWarning: graph_log_likelihood is experimental: lack of unit tests. It may be difficult to use and may have major changes at any time.\n",
      "  warnings.warn(direct_msg, ExperimentalWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270/270 [==============================] - 2s 9ms/step - loss: 3.4443\n",
      "Epoch 2/40\n",
      "270/270 [==============================] - 2s 6ms/step - loss: 3.1602\n",
      "Epoch 3/40\n",
      "270/270 [==============================] - 2s 6ms/step - loss: 2.8255\n",
      "Epoch 4/40\n",
      "270/270 [==============================] - 2s 6ms/step - loss: 2.3141\n",
      "Epoch 5/40\n",
      "270/270 [==============================] - 2s 6ms/step - loss: 1.7569\n",
      "Epoch 6/40\n",
      "270/270 [==============================] - 2s 6ms/step - loss: 1.3369\n",
      "Epoch 7/40\n",
      "212/270 [======================>.......] - ETA: 0s - loss: 1.1927"
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "train_gen = generator.flow(batch_size=batch_size, threads=10)\n",
    "\n",
    "history = model.fit(\n",
    "    train_gen, epochs=40, \n",
    "    verbose=1,\n",
    "    steps_per_epoch=int(len(G.nodes()) // batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Graph log likelihood loss vs epoch\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Graph log likelihood\")\n",
    "plt.grid()\n",
    "plt.plot(history.history['loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Embeddings\n",
    "\n",
    "Now we use TSNE to visualize the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodelist = list(G.nodes())\n",
    "\n",
    "inputs = layers.Input(batch_shape=(len(nodelist),), dtype='int64')\n",
    "left_out = model.get_layer('LEFT_EMBEDDINGS')(inputs)\n",
    "right_out = model.get_layer('RIGHT_EMBEDDINGS')(inputs)\n",
    "out = layers.concatenate([left_out, right_out])\n",
    "\n",
    "embedding_model = Model(inputs=inputs, outputs=out)\n",
    "embeddings = embedding_model.predict(np.arange(len(nodelist)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_data = pd.read_csv(os.path.join(data_dir, \"cora.content\"), sep='\\t', header=None)\n",
    "node_data.set_index(0, inplace=True)\n",
    "\n",
    "labels = node_data[node_data.columns[-1]].loc[nodelist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "target_encoding = OneHotEncoder(sparse=False)\n",
    "\n",
    "transform = TSNE\n",
    "\n",
    "trans = transform(n_components=2)\n",
    "emb_transformed = pd.DataFrame(trans.fit_transform(embeddings), index=nodelist)\n",
    "\n",
    "label_vectors = target_encoding.fit_transform(labels.values.reshape(-1, 1))\n",
    "emb_transformed['label'] = np.argmax(label_vectors, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.7\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7,7))\n",
    "ax.scatter(emb_transformed[0], emb_transformed[1], c=emb_transformed['label'].astype(\"category\"), \n",
    "            cmap=\"jet\", alpha=alpha)\n",
    "\n",
    "ax.set(aspect=\"equal\", xlabel=\"$X_1$\", ylabel=\"$X_2$\")\n",
    "plt.title('{} visualization of GloVe embeddings for cora dataset'.format(transform.__name__))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "Cora node classification. Currently this section is especially messy but is here to demonstrate that the node embeddings yield a high classification accuray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_node_idx = np.random.choice(np.arange(label_vectors.shape[0]), size=300, replace=False)\n",
    "test_node_idx = [i for i in range(label_vectors.shape[0]) if not i in train_node_idx]\n",
    "training_labels = label_vectors.copy()\n",
    "training_labels[test_node_idx] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = embeddings.shape[1] // 2\n",
    "\n",
    "predictions = np.dot(\n",
    "    np.exp(np.dot(embeddings[:, :d], embeddings[:, d:].transpose())),\n",
    "    training_labels\n",
    ")\n",
    "\n",
    "np.mean(np.argmax(predictions[test_node_idx], 1) == np.argmax(label_vectors[test_node_idx], 1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
